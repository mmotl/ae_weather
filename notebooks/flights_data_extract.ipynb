{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb9f26d9",
   "metadata": {},
   "source": [
    "### Installing packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f8c30f",
   "metadata": {},
   "source": [
    "### Defining ANSI codes for colored text prints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f87a168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSI escape codes, to make log prints nicer\n",
    "RED = \"\\033[31m\"\n",
    "GREEN = \"\\033[32m\"\n",
    "YELLOW = \"\\033[33m\"\n",
    "BLUE = \"\\033[34m\"\n",
    "BOLD = \"\\033[1m\"\n",
    "ITALIC = \"\\x1B[3m\"\n",
    "UNDERLINED = \"\\033[4m\"\n",
    "RESET = \"\\033[0m\"\n",
    "WHITE_BG    = \"\\x1b[47m\\033[30m\" # adding \\033[30m makes text black\n",
    "GREEN_BG    = \"\\x1b[102m\\033[30m\" # adding \\033[30m makes text black\n",
    "\n",
    "# https://jakob-bagterp.github.io/colorist-for-python/ansi-escape-codes/standard-16-colors/#bright-colors_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac47b11",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c263cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from natsort import os_sorted\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import zipfile\n",
    "import warnings \n",
    "import urllib3\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "from sqlalchemy import create_engine, types, text\n",
    "\n",
    "# we'll suppress the \"missing SSL certificate\" warnings while downloading files\n",
    "warnings.simplefilter(\"ignore\", urllib3.exceptions.InsecureRequestWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e7b948",
   "metadata": {},
   "source": [
    "## Data Download\n",
    "**Sources**  \n",
    ">Raw Data: https://transtats.bts.gov/PREZIP/  \n",
    ">Website: https://transtats.bts.gov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd964f81",
   "metadata": {},
   "source": [
    "#### working scenario: \n",
    "1. choose a time period for your flights data<br>**NOTE:** usually latest month available is = now - 3 months\n",
    "2. in the first cell: \n",
    "    - update `start` for the start date\n",
    "    - update `length` for the number of month \n",
    "3. execute all other cells in this notebook\n",
    "   <br>**NOTE:** the steps are optimized for multiple months period, but would also work for 1 month  \n",
    "  \n",
    "<details>\n",
    "<summary style=\"color:grey\">all steps explained</summary>\n",
    "\n",
    "1. decide on the period and update `start` and `length` variables\n",
    "2. if not yet created, add 2 folders inside `\\da-analytics-engineering-project\\` repo:\n",
    "     - `downloads`\n",
    "     - and `downloads/extracted`\n",
    "3. choose the time period for the flights data (starting month, total number of months)    \n",
    "4. under the [transtats URL](https://transtats.bts.gov/PREZIP/) above find files names starting with  \n",
    "`\"On_Time_Reporting_Carrier_On_Time_Performance_1987_present_####_##.zip\"`  \n",
    "- each ZIP file contains a CSV file for **one month** of data (indicated as ####_##)  \n",
    "- download desired zipfiles to the `downloads` folder  \n",
    "5. extract the CSV files into the `downloads/extracted` folder\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12e5994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Decide on starting month and total number of months\n",
    "start = '07.2021' # Enter the starting month and the year (MM.YYYY)\n",
    "length = 3 # How many months do you need?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fec33d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create folders for the zip files download and for the CSV-files extraction\n",
    "os.makedirs('./downloads/extracted', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8078a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create a list of months for the flight\n",
    "\n",
    "# Generate list of MM.YYYY values for one year\n",
    "def generate_year_list(start, length):\n",
    "    start_date = datetime.strptime(start, '%m.%Y')\n",
    "    return [f\"{dt.year}_{dt.month}\" for dt in\n",
    "        (start_date + timedelta(days=31 * i) for i in range(length))]\n",
    "\n",
    "# MM_YYYY values for the period lenght\n",
    "year_month_list = generate_year_list(start, length)\n",
    "\n",
    "print(year_month_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e5ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Download ZIP files (~35 seconds per one file)\n",
    "\n",
    "# Define the URL of the ZIP file\n",
    "base_url = 'https://transtats.bts.gov/PREZIP/'\n",
    "download_time = timedelta(0) # for time logging\n",
    "disk_space_zip = 0\n",
    "\n",
    "for year_month in year_month_list:\n",
    "\n",
    "    # Define the URL of the ZIP file and the CSV file\n",
    "    zip_name = f'On_Time_Reporting_Carrier_On_Time_Performance_1987_present_{year_month}.zip'\n",
    "\n",
    "    print(f'\\n ‚è≥ This should take {RED}~35 seconds...{RESET}\\n\\n    ‚¨áÔ∏è {BLUE}downloading:{RESET} {zip_name}')\n",
    "    print(f'    üêå {YELLOW}wait for it...{RESET}', end='\\r')\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # Send a HTTP request to the specified URL and save the response content\n",
    "    response = requests.get(base_url+zip_name, verify=False) # we ignore the SSL certificate warnings\n",
    "\n",
    "    with open(f'./downloads/{zip_name}', 'wb') as file: # save the ZIP in \"downloads folder\"\n",
    "        file.write(response.content)\n",
    "        print(f'    ‚úÖ {GREEN}file saved:{RESET} {zip_name}', end=' ')\n",
    "    \n",
    "    # assessing the size of the downloaded file\n",
    "    file_size = os.path.getsize(f'./downloads/{zip_name}') \n",
    "    size_in_mb = file_size / (1024 ** 2) \n",
    "    print(f'{GREEN}({size_in_mb:.2f} MB){RESET}\\n')\n",
    "    disk_space_zip += file_size\n",
    "\n",
    "    # just some fun with basic time logging  \n",
    "    end_time = datetime.now()\n",
    "    time_difference = end_time - start_time\n",
    "    download_time = download_time + time_difference\n",
    "    if (time_difference.seconds // 60) < 1:\n",
    "        print(f' ü¶ä Actually it took: {YELLOW}{time_difference.seconds % 60} seconds\\n{RESET}','-'*80)\n",
    "    else:\n",
    "        print(f' ü¶ä Actually it took: {YELLOW}{time_difference.seconds // 60} minutes and {time_difference.seconds % 60} seconds\\n{RESET}','-'*80)\n",
    "print(f' ü¶ä Total Download Time: {YELLOW}{download_time.seconds // 60} minutes and {download_time.seconds % 60} seconds\\n{RESET}')\n",
    "print(f' üêπ Used Disk Space: {GREEN}({(disk_space_zip / (1024 ** 2)):.2f} MB){RESET}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b632e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Extracting CSV files only\n",
    "\n",
    "disk_space_csv = 0\n",
    "\n",
    "for year_month in year_month_list:\n",
    "\n",
    "    # Define the name of the ZIP file and the CSV file\n",
    "    zip_name = f'On_Time_Reporting_Carrier_On_Time_Performance_1987_present_{year_month}.zip'\n",
    "    csv_name = f'On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_{year_month}.csv'\n",
    "\n",
    "    # Open the downloaded ZIP file\n",
    "    with zipfile.ZipFile(f'./downloads/{zip_name}', 'r') as zip_ref:\n",
    "        # Extract the CSV file\n",
    "        zip_ref.extract(csv_name, path='./downloads/extracted/') # save the CSV in \"downloads folder\"\n",
    "        print(f'    üçå extracted \"{csv_name}', end=' ')\n",
    "        \n",
    "    # assessing the size of the extracted file\n",
    "    file_size = os.path.getsize(f'./downloads/extracted/{csv_name}') \n",
    "    size_in_mb = file_size / (1024 ** 2) \n",
    "    print(f\"{GREEN}({size_in_mb:.2f} MB){RESET}\\n\")\n",
    "    disk_space_csv += file_size\n",
    "\n",
    "print('-'*80,f'\\n üêπ Used Disk Space: {GREEN}({(disk_space_csv / (1024 ** 2)):.2f} MB){RESET}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba185be",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14184ee",
   "metadata": {},
   "source": [
    "### 1. adding all CSV file names to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd66b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all file names from the \"extracted\" folder to a list\n",
    "file_names = os.listdir('./downloads/extracted/')\n",
    "\n",
    "# make sure only the data files are in the list\n",
    "file_names_unordered = [fname for fname in file_names if fname.startswith(\"On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_\")]\n",
    "\n",
    "# using os_sorted function (from natsort) - able to sort strings with numbers ['2','1','11']\n",
    "# sorted(['2','1','11']) # for comparison\n",
    "data_files = os_sorted(file_names_unordered)\n",
    "\n",
    "data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 110 columns in each CSV...\n",
    "file_check = pd.read_csv(f'./downloads/extracted/{data_files[0]}', low_memory = False)\n",
    "file_check.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a74cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original column names are not optimal and need renaming...\n",
    "print(file_check.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8b3091",
   "metadata": {},
   "source": [
    "### 2. Defining functions\n",
    "<font size=4>\n",
    "<ul><li>column filter<li>renaming columns<li>changing data types</ul>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c516d088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns to keep\n",
    "def cols_to_keep(flights_raw):\n",
    "    columns_to_keep = [\n",
    "        \"FlightDate\",\n",
    "        \"DepTime\",\n",
    "        \"CRSDepTime\",\n",
    "        \"DepDelay\",\n",
    "        \"ArrTime\",\n",
    "        \"CRSArrTime\",\n",
    "        \"ArrDelay\",\n",
    "        \"Reporting_Airline\",\n",
    "        \"Tail_Number\",\n",
    "        \"Flight_Number_Reporting_Airline\",\n",
    "        \"Origin\",\n",
    "        \"Dest\",\n",
    "        \"AirTime\",\n",
    "        \"ActualElapsedTime\",\n",
    "        \"Distance\",\n",
    "        \"Cancelled\",\n",
    "        \"Diverted\",\n",
    "    ]\n",
    "    flights = flights_raw.loc[:, columns_to_keep]\n",
    "    return flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d886a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "def rename_cols(flights):\n",
    "    new_column_names = {\n",
    "        'FlightDate': 'flight_date',\n",
    "        'DepTime': 'dep_time',\n",
    "        'CRSDepTime': 'sched_dep_time',\n",
    "        'DepDelay': 'dep_delay',\n",
    "        'ArrTime': 'arr_time',\n",
    "        'CRSArrTime': 'sched_arr_time',\n",
    "        'ArrDelay': 'arr_delay',\n",
    "        'Reporting_Airline': 'airline',\n",
    "        'Tail_Number': 'tail_number',\n",
    "        'Flight_Number_Reporting_Airline': 'flight_number',\n",
    "        'Origin': 'origin',\n",
    "        'Dest': 'dest',\n",
    "        'AirTime': 'air_time',\n",
    "        'ActualElapsedTime': 'actual_elapsed_time',\n",
    "        'Distance': 'distance',\n",
    "        'Cancelled': 'cancelled',\n",
    "        'Diverted': 'diverted'\n",
    "    }\n",
    "    flights.rename(columns=new_column_names, inplace=True)\n",
    "    return flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffc11e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change datatype\n",
    "def change_dtypes(flights):\n",
    "    types_change = {\n",
    "        'flight_date': 'datetime64[ns]',\n",
    "        'dep_time': 'Int16',\n",
    "        'sched_dep_time': 'Int16',\n",
    "        'dep_delay': 'Int16',\n",
    "        'arr_time': 'Int16',\n",
    "        'sched_arr_time': 'Int16',\n",
    "        'arr_delay': 'Int16',\n",
    "        'airline': 'O',\n",
    "        'tail_number': 'O',\n",
    "        'flight_number': 'Int16',\n",
    "        'origin': 'O',\n",
    "        'dest': 'O',\n",
    "        'air_time': 'Int16',\n",
    "        'actual_elapsed_time': 'Int16',\n",
    "        'distance': 'Int16',\n",
    "        'cancelled': 'Int16',\n",
    "        'diverted': 'Int16'\n",
    "    }\n",
    "    flights = flights.astype(types_change)\n",
    "    return flights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10147935",
   "metadata": {},
   "source": [
    "### 3. for-loop over the `data_files` list:\n",
    "<font size=4>\n",
    "<ol>\n",
    "<li>reading a CSV as dataframe\n",
    "<li>filtering columns\n",
    "<li>renaming columns\n",
    "<li>changing data types\n",
    "<li>append dataframe to a list of dataframes\n",
    "</ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e29fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198f0dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list for separate dataframes\n",
    "flights_list = []\n",
    "\n",
    "#  loop over the extracted csv files and execute functions \n",
    "for file in data_files:\n",
    "    print(file)\n",
    "    # 1. read as a dataframe\n",
    "    print('reading...', end=\" \")\n",
    "    flights_raw = pd.read_csv(f'./downloads/extracted/{file}', low_memory = False) \n",
    "\n",
    "    # 2.select columns to keep\n",
    "    flights_select = cols_to_keep(flights_raw) \n",
    "    print('filter colums...', end=\" \")\n",
    "    \n",
    "    # 3. rename columns\n",
    "    flights_rename = rename_cols(flights_select) \n",
    "    print('rename colums...', end=\" \")\n",
    "\n",
    "    # 4. change data types\n",
    "    flights_dtypes = change_dtypes(flights_rename) \n",
    "    print('change dtypes...', end=\" \")\n",
    "    \n",
    "    # 5. add to the list of dateframes\n",
    "    flights_list.append(flights_dtypes) \n",
    "    print(f'‚úÖ {GREEN}appended to flight_list{RESET}\\n')\n",
    "    \n",
    "print(f'Done. The list has {len(flights_list)} elements')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0469d04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. concatenate the list of dataframes to a one dataframe\n",
    "flights_all = pd.concat(flights_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df149230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort dataframe \n",
    "flights_all.sort_values(['flight_date','sched_dep_time'], inplace=True)\n",
    "flights_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3d9f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# countercheck the time period\n",
    "flights_all['flight_date'].min(), flights_all['flight_date'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f082209",
   "metadata": {},
   "source": [
    "# Saving the combined dataset<p><font size=5>(just as a backup)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba65a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the file name for the combined CSV file (using period's first and last month)\n",
    "output_file_name = f'flights_from_{year_month_list[0]}_until_{year_month_list[-1]}.csv'\n",
    "output_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb5f724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder 'data'\n",
    "os.makedirs('./data', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790d6ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "flights_all.to_csv(f'./data/{output_file_name}', index=False)\n",
    "\n",
    "print(f' ‚úÖ {GREEN}Combined Dataset Saved:{RESET} {output_file_name}', end=' ')\n",
    "\n",
    "# assessing the size of the extracted file\n",
    "file_size = os.path.getsize(f'./data/{output_file_name}') \n",
    "size_in_mb = file_size / (1024 ** 2) \n",
    "print(f\"{GREEN}({size_in_mb:.2f} MB){RESET}\\n\")\n",
    "\n",
    "# Get the absolute path\n",
    "absolute_path = os.path.abspath(f'./data/{output_file_name}')\n",
    "print(absolute_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nf_sql2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
